{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules and Load Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import Sequential\n",
    "from keras.src.callbacks import History\n",
    "from keras.src.layers import LSTM, Dense\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(data_size:int=100, path:str=\"..\\Data\\AAPL_stock_prices.csv\", delimeter: str = ',', from_end: bool = True, date_column: str = 'Date', target_column: str = 'Close') -> tuple[np.ndarray, MinMaxScaler, pd.DataFrame]:\n",
    "    df = pd.read_csv(path, delimiter=',')\n",
    "    df = df.iloc[-data_size:, :] if from_end else df.iloc[:data_size, :]\n",
    "    dates = pd.to_datetime(df[date_column])\n",
    "    df.drop(columns=[date_column], inplace=True)\n",
    "    df.index = dates\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    merged_data = np.hstack((df[target_column].values.reshape(-1, 1), df.drop(columns=[target_column]).values))\n",
    "    return scaler.fit_transform(merged_data), scaler, df\n",
    "\n",
    "def create_dataset(data: np.ndarray, time_step: int=10, output_window_size: int = 1) -> tuple[np.ndarray, np.ndarray]:\n",
    "    X, Y = [], []\n",
    "    #(len, window_size, features)\n",
    "    for i in range(len(data) - time_step):\n",
    "        # Define the range of input sequences\n",
    "        input_end_index = i + time_step\n",
    "        \n",
    "        # Define the range of output sequences\n",
    "        output_end_index = input_end_index + output_window_size\n",
    "        \n",
    "        # Ensure that the dataset is within bounds\n",
    "        if output_end_index > len(data)-1:\n",
    "            break\n",
    "            \n",
    "        # Extract input and output parts of the pattern\n",
    "        seq_x, seq_y = data[i:input_end_index], data[output_end_index]\n",
    "        \n",
    "        # Append the parts\n",
    "        X.append(seq_x)\n",
    "        Y.append(seq_y)\n",
    "\n",
    "    return np.array(X), np.array(Y)  \n",
    "\n",
    "def split_data(X: np.ndarray, Y: np.ndarray, train_size: float = 0.7, random_state: int = 42) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    # X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = train_size, random_state = random_state)\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "def create_model(input_shape: tuple, layers_with_units: list[int] = [128,128,64], optimizer: str = 'adam', loss: str = 'mean_squared_error') -> Sequential:\n",
    "    # Create the LSTM model\n",
    "    model = Sequential()\n",
    "    for layer in layers_with_units[:-1]:\n",
    "        model.add(LSTM(layer, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(LSTM(layers_with_units[-1], return_sequences=False))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer = optimizer, loss = loss)\n",
    "    return model\n",
    "\n",
    "def show_loss(history: History):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(history.history['loss'], label='Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Value Loss')\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def predict(X_train, X_test, model: Sequential) -> tuple[np.ndarray, np.ndarray]:\n",
    "    return model.predict(X_train), model.predict(X_test)\n",
    "\n",
    "# Inverse transform the predictions\n",
    "def update_data_to_inverse(predicted_data: np.ndarray, scaler: MinMaxScaler, target_column_index: int = 0, feature_number: int = 6):\n",
    "    new_dataset = np.zeros(shape=(len(predicted_data), feature_number))\n",
    "    new_dataset[:,target_column_index] = predicted_data.flatten()\n",
    "    return scaler.inverse_transform(new_dataset)[:, target_column_index].reshape(-1, 1)\n",
    "\n",
    "def calculate_scores(Y_train: np.ndarray, Y_test: np.ndarray, train_predict: np.ndarray, test_predict: np.ndarray, target_column_index: int = 0, print_results : bool = True) -> dict:\n",
    "    # Calculate MSE\n",
    "    train_mse = mean_squared_error(Y_train[:, target_column_index].reshape(-1, 1), train_predict)\n",
    "    test_mse = mean_squared_error(Y_test[:, target_column_index].reshape(-1, 1), test_predict)\n",
    "\n",
    "    # Calculate R2 score\n",
    "    train_r2 = r2_score(Y_train[:, target_column_index].reshape(-1, 1), train_predict)\n",
    "    test_r2 = r2_score(Y_test[:, target_column_index].reshape(-1, 1), test_predict)\n",
    "\n",
    "    if print_results:\n",
    "        print(f\"Train MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}\")\n",
    "        print(f\"Train R2 Score: {train_r2:.4f}, Test R2 Score: {test_r2:.4f}\")\n",
    "    return {\"mse\": {\"test\": test_mse, \"train\": train_mse}, \"r2\": {\"test\": test_r2, \"train\": train_r2}}\n",
    "\n",
    "def draw_graph(scaler: MinMaxScaler, time_step: int, target_column_index: int, train_predict: np.ndarray, test_predict: np.ndarray, scaled_data: np.ndarray):\n",
    "    print(f\"{time_step=}, {X.shape=}, {(len(train_predict) + time_step)=}\")\n",
    "    print(f\"{test_predict.shape=}, {train_predict.shape=}, {scaled_data.shape=}\")\n",
    "\n",
    "    # Plot the predictions\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(scaler.inverse_transform(scaled_data)[:, target_column_index], label='Original Data')\n",
    "    train_predict_plot = np.empty_like(scaled_data[:, target_column_index]).reshape(-1, 1)\n",
    "    train_predict_plot[:, :] = np.nan\n",
    "    train_predict_plot[time_step:len(train_predict) + time_step, :] = train_predict\n",
    "    plt.plot(train_predict_plot, label='Training Predictions')\n",
    "\n",
    "    test_predict_plot = np.empty_like(scaled_data[:, target_column_index]).reshape(-1, 1)\n",
    "    test_predict_plot[:, :] = np.nan\n",
    "    test_predict_plot[len(train_predict) + time_step:len(scaled_data[:, target_column_index]) - 1, :] = test_predict\n",
    "    plt.plot(test_predict_plot, label='Testing Predictions')\n",
    "\n",
    "    plt.title('Time Series Prediction')\n",
    "    plt.legend()\n",
    "    plt.show()    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, scaler, *_ = get_df(data_size=1000)\n",
    "X: np.ndarray\n",
    "Y: np.ndarray\n",
    "X, Y = create_dataset(data=data)\n",
    "X_train, X_test, Y_train, Y_test = split_data(X, Y)\n",
    "print(f\"{X_train.shape=}\\n{X_test.shape=}\\n{Y_train.shape=}\\n{Y_test.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(input_shape=(X_train.shape[1], X_train.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=50, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict, test_predict = predict(X_train, X_test, model)\n",
    "print(f\"{train_predict.shape=}, {test_predict.shape=}, {Y_train.shape=}, {Y_test.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict = update_data_to_inverse(train_predict, scaler=scaler, feature_number=Y_train.shape[1])\n",
    "test_predict = update_data_to_inverse(test_predict, scaler=scaler, feature_number=Y_train.shape[1])\n",
    "Y_train = scaler.inverse_transform(Y_train)\n",
    "Y_test = scaler.inverse_transform(Y_test)\n",
    "\n",
    "print(f\"{train_predict.shape=}, {test_predict.shape=}, {Y_train.shape=}, {Y_test.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_scores(Y_train = Y_train , Y_test = Y_test , train_predict = train_predict , test_predict = test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(scaler, 10, 0, train_predict, test_predict, scaled_data=data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stockprice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
